# 基础服务容器部署

## mysql

```yaml
version: "3.7"
networks:
  basic:
    name: basic
services:
  mysql:
    restart: always
    image: mysql:8.0.31-debian
    #   image: mysql:5.7.18
    container_name: mysql
    networks:
      - basic
    volumes:
      - ./mysql/mydir:/mydir
      - ./mysql/datadir:/var/lib/mysql
      - ./mysql/conf/my.cnf:/etc/my.cnf
      - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - ./mysql/source:/docker-entrypoint-initdb.d
    environment:
      - "MYSQL_ROOT_PASSWORD=123456"
      - "MYSQL_DATABASE=basic"
      - "TZ=Asia/Shanghai"
    ports:
      - '3306:3306'
    command: [ "--lower_case_table_names=1" ]

```

my.cnf

```text
[mysqld]
user=mysql
default-storage-engine=INNODB
character-set-server=utf8
character-set-client-handshake=FALSE
collation-server=utf8_general_ci
init_connect='SET NAMES utf8'
skip-host-cache
skip-name-resolve
performance_schema_max_table_instances=400
table_definition_cache=400
table_open_cache=256
performance_schema = off
max_connections = 2000
max_user_connections= 500
interactive_timeout = 86400
wait_timeout = 86400
sql_mode='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION'
[client]
default-character-set=utf8
[mysql]
default-character-set=utf8


```

## redis

```yaml
version: "3.7"
networks:
  basic:
    name: basic
services:
  redis:
    restart: always
    networks:
      - basic
    #   image: redis:latest
    image: redis:7.2.0-alpine3.18
    container_name: redis
    volumes:
      - ./redis/datadir:/data
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf
      - ./redis/logs:/logs
      - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    command: redis-server --requirepass 123456
    ports:
      - '6379:6379'
```

redis.conf

```text
# Note on units: when memory size is needed, it is possible to specifiy
# it in the usual form of 1k 5GB 4M and so forth:
#
# 1k => 1000 bytes
# 1kb => 1024 bytes
# 1m => 1000000 bytes
# 1mb => 1024*1024 bytes
# 1g => 1000000000 bytes
# 1gb => 1024*1024*1024 bytes
#
# units are case insensitive so 1GB 1Gb 1gB are all the same.
 
# Redis默认不是以守护进程的方式运行，可以通过该配置项修改，使用yes启用守护进程
# 启用守护进程后，Redis会把pid写到一个pidfile中，在/var/run/redis.pid
daemonize yes
 
# 当Redis以守护进程方式运行时，Redis默认会把pid写入/var/run/redis.pid文件，可以通过pidfile指定
pidfile /var/run/redis.pid
 
# 指定Redis监听端口，默认端口为6379
# 如果指定0端口，表示Redis不监听TCP连接
port 6379
 
# 绑定的主机地址
# 你可以绑定单一接口，如果没有绑定，所有接口都会监听到来的连接
# bind 127.0.0.1
 
# Specify the path for the unix socket that will be used to listen for
# incoming connections. There is no default, so Redis will not listen
# on a unix socket when not specified.
#
# unixsocket /tmp/redis.sock
# unixsocketperm 755
 
# 当客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能
timeout 0
 
# 指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose
# debug (很多信息, 对开发／测试比较有用)
# verbose (many rarely useful info, but not a mess like the debug level)
# notice (moderately verbose, what you want in production probably)
# warning (only very important / critical messages are logged)
loglevel verbose
 
# 日志记录方式，默认为标准输出，如果配置为redis为守护进程方式运行，而这里又配置为标准输出，则日志将会发送给/dev/null
logfile /logs/redis.log
 
# To enable logging to the system logger, just set 'syslog-enabled' to yes,
# and optionally update the other syslog parameters to suit your needs.
# syslog-enabled no
 
# Specify the syslog identity.
# syslog-ident redis
 
# Specify the syslog facility.  Must be USER or between LOCAL0-LOCAL7.
# syslog-facility local0
 
# 设置数据库的数量，默认数据库为0，可以使用select <dbid>命令在连接上指定数据库id
# dbid是从0到‘databases’-1的数目
databases 16
 
################################ SNAPSHOTTING  #################################
# 指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合
# Save the DB on disk:
#
#   save <seconds> <changes>
#
#   Will save the DB if both the given number of seconds and the given
#   number of write operations against the DB occurred.
#
#   满足以下条件将会同步数据:
#   900秒（15分钟）内有1个更改
#   300秒（5分钟）内有10个更改
#   60秒内有10000个更改
#   Note: 可以把所有“save”行注释掉，这样就取消同步操作了
 
save 900 1
save 300 10
save 60 10000
 
# 指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间，可以关闭该选项，但会导致数据库文件变的巨大
rdbcompression yes
 
# 指定本地数据库文件名，默认值为dump.rdb
dbfilename dump.rdb
 
# 工作目录.
# 指定本地数据库存放目录，文件名由上一个dbfilename配置项指定
#
# Also the Append Only File will be created inside this directory.
#
# 注意，这里只能指定一个目录，不能指定文件名
dir ./
 
################################# REPLICATION #################################
 
# 主从复制。使用slaveof从 Redis服务器复制一个Redis实例。注意，该配置仅限于当前slave有效
# so for example it is possible to configure the slave to save the DB with a
# different interval, or to listen to another port, and so on.
# 设置当本机为slav服务时，设置master服务的ip地址及端口，在Redis启动时，它会自动从master进行数据同步
# slaveof <masterip> <masterport>
 
 
# 当master服务设置了密码保护时，slav服务连接master的密码
# 下文的“requirepass”配置项可以指定密码
# masterauth <master-password>
 
# When a slave lost the connection with the master, or when the replication
# is still in progress, the slave can act in two different ways:
#
# 1) if slave-serve-stale-data is set to 'yes' (the default) the slave will
#    still reply to client requests, possibly with out of data data, or the
#    data set may just be empty if this is the first synchronization.
#
# 2) if slave-serve-stale data is set to 'no' the slave will reply with
#    an error "SYNC with master in progress" to all the kind of commands
#    but to INFO and SLAVEOF.
#
slave-serve-stale-data yes
 
# Slaves send PINGs to server in a predefined interval. It's possible to change
# this interval with the repl_ping_slave_period option. The default value is 10
# seconds.
#
# repl-ping-slave-period 10
 
# The following option sets a timeout for both Bulk transfer I/O timeout and
# master data or ping response timeout. The default value is 60 seconds.
#
# It is important to make sure that this value is greater than the value
# specified for repl-ping-slave-period otherwise a timeout will be detected
# every time there is low traffic between the master and the slave.
#
# repl-timeout 60
 
################################## SECURITY ###################################
 
# Warning: since Redis is pretty fast an outside user can try up to
# 150k passwords per second against a good box. This means that you should
# use a very strong password otherwise it will be very easy to break.
# 设置Redis连接密码，如果配置了连接密码，客户端在连接Redis时需要通过auth <password>命令提供密码，默认关闭
requirepass yourpassword
# Command renaming.
#
# It is possilbe to change the name of dangerous commands in a shared
# environment. For instance the CONFIG command may be renamed into something
# of hard to guess so that it will be still available for internal-use
# tools but not available for general clients.
#
# Example:
#
# rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52
#
# It is also possilbe to completely kill a command renaming it into
# an empty string:
#
# rename-command CONFIG ""
 
################################### LIMITS ####################################
 
# 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，
# 如果设置maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max Number of clients reached错误信息
# maxclients 128
 
# Don't use more memory than the specified amount of bytes.
# When the memory limit is reached Redis will try to remove keys with an
# EXPIRE set. It will try to start freeing keys that are going to expire
# in little time and preserve keys with a longer time to live.
# Redis will also try to remove objects from free lists if possible.
#
# If all this fails, Redis will start to reply with errors to commands
# that will use more memory, like SET, LPUSH, and so on, and will continue
# to reply to most read-only commands like GET.
#
# WARNING: maxmemory can be a good idea mainly if you want to use Redis as a
# 'state' server or cache, not as a real DB. When Redis is used as a real
# database the memory usage will grow over the weeks, it will be obvious if
# it is going to use too much memory in the long run, and you'll have the time
# to upgrade. With maxmemory after the limit is reached you'll start to get
# errors for write operations, and this may even lead to DB inconsistency.
# 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key，
# 当此方法处理后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。
# Redis新的vm机制，会把Key存放内存，Value会存放在swap区
# maxmemory <bytes>
 
# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory
# is reached? You can select among five behavior:
#
# volatile-lru -> remove the key with an expire set using an LRU algorithm
# allkeys-lru -> remove any key accordingly to the LRU algorithm
# volatile-random -> remove a random key with an expire set
# allkeys->random -> remove a random key, any key
# volatile-ttl -> remove the key with the nearest expire time (minor TTL)
# noeviction -> don't expire at all, just return an error on write operations
#
# Note: with all the kind of policies, Redis will return an error on write
#       operations, when there are not suitable keys for eviction.
#
#       At the date of writing this commands are: set setnx setex append
#       incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd
#       sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby
#       zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby
#       getset mset msetnx exec sort
#
# The default is:
#
# maxmemory-policy volatile-lru
 
# LRU and minimal TTL algorithms are not precise algorithms but approximated
# algorithms (in order to save memory), so you can select as well the sample
# size to check. For instance for default Redis will check three keys and
# pick the one that was used less recently, you can change the sample size
# using the following configuration directive.
#
# maxmemory-samples 3
 
############################## APPEND ONLY MODE ###############################
 
#
# Note that you can have both the async dumps and the append only file if you
# like (you have to comment the "save" statements above to disable the dumps).
# Still if append only mode is enabled Redis will load the data from the
# log file at startup ignoring the dump.rdb file.
# 指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。
# 因为redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为no
# IMPORTANT: Check the BGREWRITEAOF to check how to rewrite the append
# log file in background when it gets too big.
 
appendonly yes
 
# 指定更新日志文件名，默认为appendonly.aof
# appendfilename appendonly.aof
 
# The fsync() call tells the Operating System to actually write data on disk
# instead to wait for more data in the output buffer. Some OS will really flush
# data on disk, some other OS will just try to do it ASAP.
 
# 指定更新日志条件，共有3个可选值：
# no:表示等操作系统进行数据缓存同步到磁盘（快）
# always:表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全）
# everysec:表示每秒同步一次（折衷，默认值）
 
appendfsync everysec
# appendfsync no
 
# When the AOF fsync policy is set to always or everysec, and a background
# saving process (a background save or AOF log background rewriting) is
# performing a lot of I/O against the disk, in some Linux configurations
# Redis may block too long on the fsync() call. Note that there is no fix for
# this currently, as even performing fsync in a different thread will block
# our synchronous write(2) call.
#
# In order to mitigate this problem it's possible to use the following option
# that will prevent fsync() from being called in the main process while a
# BGSAVE or BGREWRITEAOF is in progress.
#
# This means that while another child is saving the durability of Redis is
# the same as "appendfsync none", that in pratical terms means that it is
# possible to lost up to 30 seconds of log in the worst scenario (with the
# default Linux settings).
#
# If you have latency problems turn this to "yes". Otherwise leave it as
# "no" that is the safest pick from the point of view of durability.
no-appendfsync-on-rewrite no
 
# Automatic rewrite of the append only file.
# Redis is able to automatically rewrite the log file implicitly calling
# BGREWRITEAOF when the AOF log size will growth by the specified percentage.
#
# This is how it works: Redis remembers the size of the AOF file after the
# latest rewrite (or if no rewrite happened since the restart, the size of
# the AOF at startup is used).
#
# This base size is compared to the current size. If the current size is
# bigger than the specified percentage, the rewrite is triggered. Also
# you need to specify a minimal size for the AOF file to be rewritten, this
# is useful to avoid rewriting the AOF file even if the percentage increase
# is reached but it is still pretty small.
#
# Specify a precentage of zero in order to disable the automatic AOF
# rewrite feature.
 
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
 
################################## SLOW LOG ###################################
 
# The Redis Slow Log is a system to log queries that exceeded a specified
# execution time. The execution time does not include the I/O operations
# like talking with the client, sending the reply and so forth,
# but just the time needed to actually execute the command (this is the only
# stage of command execution where the thread is blocked and can not serve
# other requests in the meantime).
#
# You can configure the slow log with two parameters: one tells Redis
# what is the execution time, in microseconds, to exceed in order for the
# command to get logged, and the other parameter is the length of the
# slow log. When a new command is logged the oldest one is removed from the
# queue of logged commands.
 
# The following time is expressed in microseconds, so 1000000 is equivalent
# to one second. Note that a negative number disables the slow log, while
# a value of zero forces the logging of every command.
slowlog-log-slower-than 10000
 
# There is no limit to this length. Just be aware that it will consume memory.
# You can reclaim memory used by the slow log with SLOWLOG RESET.
slowlog-max-len 1024
 
################################ VIRTUAL MEMORY ###############################
 
### WARNING! Virtual Memory is deprecated in Redis 2.4
### The use of Virtual Memory is strongly discouraged.
 
### WARNING! Virtual Memory is deprecated in Redis 2.4
### The use of Virtual Memory is strongly discouraged.
 
# Virtual Memory allows Redis to work with datasets bigger than the actual
# amount of RAM needed to hold the whole dataset in memory.
# In order to do so very used keys are taken in memory while the other keys
# are swapped into a swap file, similarly to what operating systems do
# with memory pages.
# 指定是否启用虚拟内存机制，默认值为no，
# VM机制将数据分页存放，由Redis将访问量较少的页即冷数据swap到磁盘上，访问多的页面由磁盘自动换出到内存中
# 把vm-enabled设置为yes，根据需要设置好接下来的三个VM参数，就可以启动VM了
# vm-enabled no
# vm-enabled yes
 
# This is the path of the Redis swap file. As you can guess, swap files
# can't be shared by different Redis instances, so make sure to use a swap
# file for every redis process you are running. Redis will complain if the
# swap file is already in use.
#
# Redis交换文件最好的存储是SSD（固态硬盘）
# 虚拟内存文件路径，默认值为/tmp/redis.swap，不可多个Redis实例共享
# *** WARNING *** if you are using a shared hosting the default of putting
# the swap file under /tmp is not secure. Create a dir with access granted
# only to Redis user and configure Redis to create the swap file there.
# vm-swap-file /tmp/redis.swap
 
# With vm-max-memory 0 the system will swap everything it can. Not a good
# default, just specify the max amount of RAM you can in bytes, but it's
# better to leave some margin. For instance specify an amount of RAM
# that's more or less between 60 and 80% of your free RAM.
# 将所有大于vm-max-memory的数据存入虚拟内存，无论vm-max-memory设置多少，所有索引数据都是内存存储的（Redis的索引数据就是keys）
# 也就是说当vm-max-memory设置为0的时候，其实是所有value都存在于磁盘。默认值为0
# vm-max-memory 0
 
# Redis swap文件分成了很多的page，一个对象可以保存在多个page上面，但一个page上不能被多个对象共享，vm-page-size是要根据存储的数据大小来设定的。
# 建议如果存储很多小对象，page大小最后设置为32或64bytes；如果存储很大的对象，则可以使用更大的page，如果不确定，就使用默认值
# vm-page-size 32
 
# 设置swap文件中的page数量由于页表（一种表示页面空闲或使用的bitmap）是存放在内存中的，在磁盘上每8个pages将消耗1byte的内存
# swap空间总容量为 vm-page-size * vm-pages
#
# With the default of 32-bytes memory pages and 134217728 pages Redis will
# use a 4 GB swap file, that will use 16 MB of RAM for the page table.
#
# It's better to use the smallest acceptable value for your application,
# but the default is large in order to work in most conditions.
# vm-pages 134217728
 
# Max number of VM I/O threads running at the same time.
# This threads are used to read/write data from/to swap file, since they
# also encode and decode objects from disk to memory or the reverse, a bigger
# number of threads can help with big objects even if they can't help with
# I/O itself as the physical device may not be able to couple with many
# reads/writes operations at the same time.
# 设置访问swap文件的I/O线程数，最后不要超过机器的核数，如果设置为0，那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟，默认值为4
# vm-max-threads 4
 
############################### ADVANCED CONFIG ###############################
 
# Hashes are encoded in a special way (much more memory efficient) when they
# have at max a given numer of elements, and the biggest element does not
# exceed a given threshold. You can configure this limits with the following
# configuration directives.
# 指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法
# hash-max-zipmap-entries 512
# hash-max-zipmap-value 64
 
# Similarly to hashes, small lists are also encoded in a special way in order
# to save a lot of space. The special representation is only used when
# you are under the following limits:
list-max-ziplist-entries 512
list-max-ziplist-value 64
 
# Sets have a special encoding in just one case: when a set is composed
# of just strings that happens to be integers in radix 10 in the range
# of 64 bit signed integers.
# The following configuration setting sets the limit in the size of the
# set in order to use this special memory saving encoding.
set-max-intset-entries 512
 
# Similarly to hashes and lists, sorted sets are also specially encoded in
# order to save a lot of space. This encoding is only used when the length and
# elements of a sorted set are below the following limits:
zset-max-ziplist-entries 128
zset-max-ziplist-value 64
 
# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in
# order to help rehashing the main Redis hash table (the one mapping top-level
# keys to values). The hash table implementation redis uses (see dict.c)
# performs a lazy rehashing: the more operation you run into an hash table
# that is rhashing, the more rehashing "steps" are performed, so if the
# server is idle the rehashing is never complete and some more memory is used
# by the hash table.
#
# The default is to use this millisecond 10 times every second in order to
# active rehashing the main dictionaries, freeing memory when possible.
#
# If unsure:
# use "activerehashing no" if you have hard latency requirements and it is
# not a good thing in your environment that Redis can reply form time to time
# to queries with 2 milliseconds delay.
# 指定是否激活重置哈希，默认为开启
activerehashing yes
 
################################## INCLUDES ###################################
 
# 指定包含其他的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各实例又拥有自己的特定配置文件
# include /path/to/local.conf
# include /path/to/other.conf
```

## nginx

```yaml
version: "3.7"
networks:
  basic:
    name: basic
services:
  nginx:
    restart: always
    container_name: nginx
    #   image: nginx:latest
    image: nginx:mainline-alpine3.18
    ports:
      - "8080:80"
    volumes:
      - ./nginx/conf.d/:/etc/nginx/conf.d/
      - ./nginx/conf/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/html/:/usr/share/nginx/html
      - ./nginx/logs:/var/nginx/log
      - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    networks:
      - basic
```

nginx.conf

```text
user  nginx;
worker_processes  auto;

error_log  /var/log/nginx/error.log notice;
pid        /var/run/nginx.pid;

events {
    worker_connections  1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    keepalive_timeout  65;

    gzip  on;

    include /etc/nginx/conf.d/*.conf;
}

```

default.conf

```text
server {
    listen       80;
    listen  [::]:80;
    server_name  localhost;

    access_log  /var/log/nginx/main.access.log  main;
    error_log  /var/log/nginx/main.error.log info;
    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }

    #location = /index{
    #    root    /usr/share/nginx/html;
    #    index test.html;
    #}
    
    error_page  404              /404.html;

    # redirect server error pages to the static page /50x.html
    #
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }
    location /demo {
        root /usr/share/nginx/html;
        page demo.html   
    }
    # proxy the PHP scripts to Apache listening on 127.0.0.1:80
    #
    #location ~ \.php$ {
    #    proxy_pass   http://127.0.0.1;
    #}

    # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000
    #
    #location ~ \.php$ {
    #    root           html;
    #    fastcgi_pass   127.0.0.1:9000;
    #    fastcgi_index  index.php;
    #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;
    #    include        fastcgi_params;
    #}

    # deny access to .htaccess files, if Apache's document root
    # concurs with nginx's one
    #
    #location ~ /\.ht {
    #    deny  all;
    #}
}
```

## mongodb

```yaml
version: "3.7"
networks:
  basic:
    name: basic
services:
  mongodb:
    container_name: mongodb
    image: mongo:latest
    ports:
      - "27017:27017"
    restart: always
    command: mongod
    networks:
      - basic
    environment:
      TZ: Asia/Shanghai
      MONGO_INITDB_DATABASE: basic
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: "123456"
      wiredTigerCacheSizeGB: 2
    volumes:
      - "./mongo/data:/data/db"
      - "./mongo/logs:/var/log/mongodb"
      - "/usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro"
      - ./mongo/init.d:/docker-entrypoint-initdb.d
      - /etc/timezone:/etc/timezone:ro
```

## prometheus+grafana+cadvisor+node-exporter

```yaml
version: "3.9"
networks:
  basic:
    name: basic

volumes:
  prometheus_data: { }
  grafana_storage: { }

services:
  prometheus:
    image: prom/prometheus:v2.32.1
    container_name: prometheus
    volumes:
      - ./prometheus/config/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    ports:
      - "9090:9090"
    networks:
      - basic
    restart: always
  grafana:
    image: grafana/grafana:8.3.4
    container_name: grafana
    user: "104"
    depends_on:
      - prometheus
    ports:
      - "3000:3000"
    volumes:
      - grafana_storage:/var/lib/grafana
      - ./grafana/provisioning/:/etc/grafana/provisioning/
    env_file:
      - ./grafana/config.monitoring
    restart: always
    networks:
      - basic
  cadvisor:
    image: lagoudocker/cadvisor:v0.37.0
    privileged: true
    networks:
      - basic
    container_name: cadvisor
    volumes:
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /etc/timezone:/etc/timezone:ro
    #      - /cgroup:/cgroup:ro
    ports:
      - 8080:8080
    restart: always
  node-exporter:
    image: prom/node-exporter:v1.3.1
    privileged: true
    restart: always
    networks:
      - basic
    container_name: node-exporter
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /etc/timezone:/etc/timezone:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - --collector.filesystem.ignored-mount-points
      - "^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)"
    ports:
      - 9100:9100
```

prometheus.yml

```yaml
global:
  scrape_interval: 1s
  external_labels:
    monitor: 'devopsage-monitor'

scrape_configs:
  - job_name: "prometheus"
    static_configs:
      - targets: [ "localhost:9090" ]
```

config.monitoring

```text
GF_SECURITY_ADMIN_PASSWORD=admin
GF_USERS_ALLOW_SIGN_UP=false
```

grafana.provisioning.dashboards.dashboard.yml

```yaml
apiVersion: 1

providers:
  - name: 'Prometheus'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    editable: true
    options:
      path: /etc/grafana/provisioning/dashboards
```

grafana.provisioning.dashboards.hlf-performances.json

tips: performances.json是dashboard展示的模板文件

grafana.provisioning.datasources.datasource.yml

```yaml
# config file version
apiVersion: 1

# list of datasources that should be deleted from the database
deleteDatasources:
  - name: Prometheus
    orgId: 1

# list of datasources to insert/update depending
# whats available in the database
datasources:
  # <string, required> name of the datasource. Required
  - name: Prometheus
    # <string, required> datasource type. Required
    type: prometheus
    # <string, required> access mode. direct or proxy. Required
    access: proxy
    # <int> org id. will default to orgId 1 if not specified
    orgId: 1
    # <string> url
    url: http://prometheus:9090
    # <string> database password, if used
    password:
    # <string> database user, if used
    user:
    # <string> database name, if used
    database:
    # <bool> enable/disable basic auth
    basicAuth: true
    # <string> basic auth username
    basicAuthUser: admin
    # <string> basic auth password
    basicAuthPassword: foobar
    # <bool> enable/disable with credentials headers
    withCredentials:
    # <bool> mark as default datasource. Max one per org
    isDefault: true
    # <map> fields that will be converted to json and stored in json_data
    jsonData:
      graphiteVersion: "1.1"
      tlsAuth: false
      tlsAuthWithCACert: false
    # <string> json object of data that will be encrypted.
    secureJsonData:
      tlsCACert: "..."
      tlsClientCert: "..."
      tlsClientKey: "..."
    version: 1
    # <bool> allow users to edit datasources from the UI.
    editable: true
```

## portainer

```yaml
version: "3.9"
networks:
  basic:
    name: basic
services:
  portainer:
    image: portainer/portainer:latest
    container_name: portainer
    networks:
      - basic
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./portainer/data:/data
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    ports:
      - 9000:9000
    restart: always
```

## elasticsearch

```yaml
version: "3.9"
networks:
  basic:
    name: basic
services:
  es:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.10.0
    container_name: es
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - 9200:9200
      - 9300:9300
    volumes:
      - ./elasticsearch/data:/usr/share/elasticsearch/data
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    networks:
      - basic
```

## ipfs

```yaml
version: "3.9"
networks:
  basic:
    name: basic
services:
  ipfs:
    image: ipfs/kubo:latest
    restart: unless-stopped
    container_name: ipfs
    volumes:
      - ./ipfs/ipfs-data:/data/ipfs
      - ./ipfs/ipfs-fuse:/ipfs
      - ./ipfs/ipns-fuse:/ipns
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    environment:
      - IPFS_PATH=/data/ipfs
    ports:
      - 4001:4001/tcp
      - 4001:4001/udp
      - 127.0.0.1:5001:5001
      - 127.0.0.1:8080:8080
    networks:
      - basic
```

## zookeeper*3+kafka

```yaml
version: "3.9"
networks:
  basic:
    name: basic
services:
  zookeeper1:
    image: wurstmeister/zookeeper:latest
    networks:
      - basic
    container_name: zookeeper1
    restart: always
    ports:
      - "2181:2181"
      - '8081:8080'
    volumes:
      - ./zookeeper/zookeeper1/data:/data
      - ./zookeeper/zookeeper1/logs:/datalog
      - ./zookeeper/zookeeper1/config:/conf
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    environment:
      - ZOO_MY_ID=1
      - ZOO_TICK_TIME=2000
      - ZOO_INIT_LIMIT=5
      - ZOO_SYNC_LIMIT=2
      - ZOO_MAX_CLIENT_CNXNS=1200
    privileged: true

  zookeeper2:
    image: wurstmeister/zookeeper:latest
    networks:
      - basic
    container_name: zookeeper2
    restart: always
    ports:
      - "2182:2181"
      - '8082:8080'
    volumes:
      - ./zookeeper/zookeeper2/data:/data
      - ./zookeeper/zookeeper2/logs:/datalog
      - ./zookeeper/zookeeper2/config:/conf
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    environment:
      - ZOO_MY_ID=2
      - ZOO_TICK_TIME=2000
      - ZOO_INIT_LIMIT=5
      - ZOO_SYNC_LIMIT=2
      - ZOO_MAX_CLIENT_CNXNS=1200
    privileged: true

  zookeeper3:
    image: wurstmeister/zookeeper:latest
    networks:
      - basic
    container_name: zookeeper3
    restart: always
    ports:
      - "2183:2181"
      - '8083:8080'
    volumes:
      - ./zookeeper/zookeeper3/data:/data
      - ./zookeeper/zookeeper3/logs:/datalog
      - ./zookeeper/zookeeper3/config:/conf
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    environment:
      - ZOO_MY_ID=3
      - ZOO_TICK_TIME=2000
      - ZOO_INIT_LIMIT=5
      - ZOO_SYNC_LIMIT=2
      - ZOO_MAX_CLIENT_CNXNS=1200
    privileged: true

  zookeeper-exporter:
    image: dabealu/zookeeper-exporter:v0.1.13
    restart: always
    container_name: zookeeper_exporter
    depends_on:
      - zookeeper1
      - zookeeper2
      - zookeeper3
    ports:
      - "9141:9141"
    command: --zk-hosts="zookeeper1:2181,zookeeper2:2181,zookeeper3:2181"
    privileged: true
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    networks:
      - basic

  kafka:
    image: wurstmeister/kafka:latest
    container_name: kafka
    networks:
      - basic
    environment:
      # 日志文件保存120个小时
      - KAFKA_LOG_RETENTION_HOURS=120
      # broker的topic分区leader接受数据的时候，允许的单条消息的最大值，默认为1M
      #      - KAFKA_MESSAGE_MAX_BYTES=10000000
      #      - KAFKA_REPLICA_FETCH_MAX_BYTES=10000000
      # broker端的leader分区在想其他follower分区复制消息时候 ，允许的单条消息的最大值
      #      - KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS=60000
      # topic的分区数
      #      - KAFKA_NUM_PARTITIONS=3
      #      - KAFKA_DELETE_RETENTION_MS=1000
      #  # Zookeeper连接地址，格式：zoo1：port1,zoo2:port2:/path
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper1:2181,zookeeper2:2181,zookeeper3:2181
      # Kafka广播地址及端口，告诉客户端，使用什么地址和端口能连接到Kafka，不指定，宿主机以外的客户端将无法连接到Kafka
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://192.168.58.131:9092
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      # 指定BrokerId，如果不指定，将会自己生成
      - KAFKA_BROKER_ID=0
      # kafaka 启动后初始化 2 partition 1 副本 名为basic的topic
      - KAFKA_CREATE_TOPICS=basic:2:1
      - KAFKA_LISTENERS=PLAINTEXT://:9092
      - KAFKA_AUTO_CREATE_TOPICS=true
    restart: always
    ports:
      - '9092:9092'
    depends_on:
      - zookeeper1
      - zookeeper2
      - zookeeper3
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./kafka/logs:/kafka
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro

  kafka-manager:
    ## 镜像：开源的web管理kafka集群的界面
    image: sheepkiller/kafka-manager:latest
    container_name: kafka-manager
    environment:
      ## 修改:宿主机IP
      ZK_HOSTS: 192.168.58.131
    ports:
      - "9009:9000"
    networks:
      - basic
```

zookeeper.zoo.cfg

Tips: 三个节点配置相同

```text
#用于接收客户端请求的端口号
clientPort=2181
#配置ZK的数据目录
dataDir=/data
#配置ZK的日志目录
dataLogDir=/datalog
#ZK中的时间配置最小但域，其他时间配置以整数倍tickTime计算
tickTime=2000
#Leader允许Follower启动时在initLimit时间内完成数据同步，单位：tickTime
initLimit=5
#Leader发送心跳包给集群中所有Follower，若Follower在syncLimit时间内没有响应，那么Leader就认为该follower已经挂掉了，单位：tickTime
syncLimit=2
autopurge.snapRetainCount=3
autopurge.purgeInterval=0
maxClientCnxns=1200
maxSessionTimeout=30000
standaloneEnabled=true
admin.enableServer=true
#ZK集群节点配置，端口号2888用于集群节点之间数据通信，端口号3888用于集群中Leader选举
server.1=zookeeper1:2888:3888;2181
server.2=zookeeper2:2888:3888;2181
server.3=zookeeper3:2888:3888;2181
4lw.commands.whitelist=*
```
